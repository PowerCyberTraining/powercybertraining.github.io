{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0253qa076f",
   "metadata": {},
   "source": [
    "# Time Series Analysis and Forecasting for Power Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Time series analysis is fundamental to power system operations. System operators rely on accurate forecasts for load scheduling, unit commitment, and reserve planning. This lesson applies time series techniques specifically to power system data, focusing on practical forecasting methods used in the industry.\n",
    "\n",
    "Power systems exhibit strong temporal patterns driven by human behavior, weather conditions, and industrial activities. Understanding these patterns and building reliable forecasts is crucial for maintaining grid stability and economic efficiency.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1. Identify and analyze temporal patterns in power system data\n",
    "2. Decompose time series to understand trend, seasonal, and residual components\n",
    "3. Apply autocorrelation analysis for model selection\n",
    "4. Implement practical forecasting methods for load and generation\n",
    "5. Evaluate forecast accuracy using industry-standard metrics\n",
    "6. Handle real-world challenges in power system forecasting\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This lesson builds on:\n",
    "- Python fundamentals (Lesson 1)\n",
    "- NumPy for numerical computation (Lesson 2)\n",
    "- Pandas for data manipulation (Lesson 3)\n",
    "- Data visualization techniques (Lesson 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqoddnwv8w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.precision', 2)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fak5dlshqat",
   "metadata": {},
   "source": [
    "## 1. Time Series Patterns in Power Systems\n",
    "\n",
    "Power system data exhibits distinct patterns that reflect the underlying physics and human behavior. Let's create and analyze realistic power system time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p23n10fkxpo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two years of hourly load data with realistic patterns\n",
    "dates = pd.date_range(start='2022-01-01', end='2023-12-31 23:00:00', freq='H')\n",
    "n_hours = len(dates)\n",
    "\n",
    "# Base load\n",
    "base_load = 5000  # MW\n",
    "\n",
    "# Long-term trend (slight growth)\n",
    "trend = np.linspace(0, 200, n_hours)  # 200 MW growth over 2 years\n",
    "\n",
    "# Annual seasonality (higher in summer and winter)\n",
    "annual_pattern = 300 * np.sin(2 * np.pi * (dates.dayofyear - 80) / 365.25)  # Peak in summer\n",
    "annual_pattern += 200 * np.sin(4 * np.pi * dates.dayofyear / 365.25)  # Secondary peak in winter\n",
    "\n",
    "# Weekly seasonality (lower on weekends)\n",
    "weekly_pattern = np.where(dates.weekday < 5, 0, -500)  # 500 MW reduction on weekends\n",
    "\n",
    "# Daily seasonality (double peak pattern)\n",
    "daily_pattern = (\n",
    "    -200 * np.cos(2 * np.pi * (dates.hour - 6) / 24) +  # Morning peak around 9 AM\n",
    "    -150 * np.cos(2 * np.pi * (dates.hour - 18) / 24)   # Evening peak around 6 PM\n",
    ")\n",
    "\n",
    "# Temperature effect (simplified)\n",
    "temperature = 20 + 10 * np.sin(2 * np.pi * (dates.dayofyear - 80) / 365.25) + \\\n",
    "              5 * np.sin(2 * np.pi * dates.hour / 24) + \\\n",
    "              np.random.normal(0, 2, n_hours)\n",
    "\n",
    "# Temperature-dependent load (cooling and heating)\n",
    "temp_load = 20 * (temperature - 20)**2  # Quadratic relationship\n",
    "\n",
    "# Random noise\n",
    "noise = np.random.normal(0, 100, n_hours)\n",
    "\n",
    "# Combine all components\n",
    "load = base_load + trend + annual_pattern + weekly_pattern + daily_pattern + temp_load + noise\n",
    "load = np.maximum(load, 3000)  # Minimum load constraint\n",
    "\n",
    "# Create DataFrame\n",
    "load_data = pd.DataFrame({\n",
    "    'datetime': dates,\n",
    "    'load': load,\n",
    "    'temperature': temperature,\n",
    "    'weekday': dates.weekday,\n",
    "    'hour': dates.hour,\n",
    "    'month': dates.month,\n",
    "    'dayofyear': dates.dayofyear\n",
    "})\n",
    "load_data.set_index('datetime', inplace=True)\n",
    "\n",
    "print(\"Generated 2 years of hourly load data\")\n",
    "print(f\"Shape: {load_data.shape}\")\n",
    "print(f\"Load range: {load_data['load'].min():.0f} - {load_data['load'].max():.0f} MW\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(load_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0kpp4g5tepi8",
   "metadata": {},
   "source": [
    "### Visualizing Time Series Patterns\n",
    "\n",
    "Let's visualize the data at different time scales to identify patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7y3f5lgyvpi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-scale visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# 1. Full two years\n",
    "ax = axes[0]\n",
    "ax.plot(load_data.index, load_data['load'], linewidth=0.5)\n",
    "ax.set_title('Two Years of Hourly Load Data')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. One month (January 2023)\n",
    "ax = axes[1]\n",
    "jan_2023 = load_data['2023-01']\n",
    "ax.plot(jan_2023.index, jan_2023['load'])\n",
    "ax.set_title('January 2023 - Monthly Pattern')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. One week (first week of July 2023)\n",
    "ax = axes[2]\n",
    "week_data = load_data['2023-07-01':'2023-07-07']\n",
    "ax.plot(week_data.index, week_data['load'], marker='o', markersize=3)\n",
    "ax.set_title('First Week of July 2023 - Weekly Pattern')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "# Add day labels\n",
    "for i, day in enumerate(['Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri']):\n",
    "    ax.axvline(week_data.index[i*24], color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.text(week_data.index[i*24 + 12], ax.get_ylim()[1]*0.95, day, ha='center')\n",
    "\n",
    "# 4. Temperature vs Load scatter\n",
    "ax = axes[3]\n",
    "scatter = ax.scatter(load_data['temperature'], load_data['load'], \n",
    "                    c=load_data['hour'], cmap='viridis', alpha=0.3, s=1)\n",
    "ax.set_xlabel('Temperature (Â°C)')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.set_title('Load vs Temperature (colored by hour of day)')\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Hour of Day')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print pattern statistics\n",
    "print(\"\\nPattern Analysis:\")\n",
    "print(f\"Average weekday load: {load_data[load_data['weekday'] < 5]['load'].mean():.0f} MW\")\n",
    "print(f\"Average weekend load: {load_data[load_data['weekday'] >= 5]['load'].mean():.0f} MW\")\n",
    "print(f\"Weekend reduction: {(1 - load_data[load_data['weekday'] >= 5]['load'].mean() / load_data[load_data['weekday'] < 5]['load'].mean()) * 100:.1f}%\")\n",
    "\n",
    "summer_load = load_data[load_data['month'].isin([6, 7, 8])]['load'].mean()\n",
    "winter_load = load_data[load_data['month'].isin([12, 1, 2])]['load'].mean()\n",
    "print(f\"\\nAverage summer load: {summer_load:.0f} MW\")\n",
    "print(f\"Average winter load: {winter_load:.0f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "it5zk3t4mwl",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1: Identify Time Series Patterns\n",
    ":class: tip\n",
    "\n",
    "Using the load data, identify and visualize:\n",
    "1. The typical daily load profile for weekdays vs weekends\n",
    "2. The monthly average load pattern across the year\n",
    "3. The relationship between temperature and load for different seasons\n",
    "4. Any special days (holidays) with unusual patterns\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "in5vdulumep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Identify and visualize time series patterns\n",
    "\n",
    "# Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfda0h8hswp",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Daily profile: weekdays vs weekends\n",
    "ax = axes[0, 0]\n",
    "weekday_profile = load_data[load_data['weekday'] < 5].groupby('hour')['load'].mean()\n",
    "weekend_profile = load_data[load_data['weekday'] >= 5].groupby('hour')['load'].mean()\n",
    "\n",
    "ax.plot(weekday_profile.index, weekday_profile.values, 'b-', linewidth=2, label='Weekday')\n",
    "ax.plot(weekend_profile.index, weekend_profile.values, 'r--', linewidth=2, label='Weekend')\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Average Load (MW)')\n",
    "ax.set_title('Daily Load Profile: Weekday vs Weekend')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(0, 24, 3))\n",
    "\n",
    "# 2. Monthly average pattern\n",
    "ax = axes[0, 1]\n",
    "monthly_avg = load_data.groupby('month')['load'].mean()\n",
    "ax.bar(monthly_avg.index, monthly_avg.values, color='skyblue', edgecolor='black')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Average Load (MW)')\n",
    "ax.set_title('Monthly Average Load Pattern')\n",
    "ax.set_xticks(range(1, 13))\n",
    "ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Temperature vs Load by season\n",
    "ax = axes[1, 0]\n",
    "seasons = {\n",
    "    'Winter': [12, 1, 2],\n",
    "    'Spring': [3, 4, 5],\n",
    "    'Summer': [6, 7, 8],\n",
    "    'Fall': [9, 10, 11]\n",
    "}\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "for (season, months), color in zip(seasons.items(), colors):\n",
    "    season_data = load_data[load_data['month'].isin(months)]\n",
    "    ax.scatter(season_data['temperature'], season_data['load'], \n",
    "              alpha=0.3, s=1, label=season, color=color)\n",
    "\n",
    "ax.set_xlabel('Temperature (Â°C)')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.set_title('Load vs Temperature by Season')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Identify anomalies (simplified - looking for low load days)\n",
    "ax = axes[1, 1]\n",
    "daily_avg = load_data.resample('D')['load'].mean()\n",
    "threshold = daily_avg.quantile(0.05)  # Bottom 5% as potential holidays\n",
    "\n",
    "anomalies = daily_avg[daily_avg < threshold]\n",
    "ax.plot(daily_avg.index, daily_avg.values, 'b-', alpha=0.5, linewidth=0.5)\n",
    "ax.scatter(anomalies.index, anomalies.values, color='red', s=50, zorder=5)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Daily Average Load (MW)')\n",
    "ax.set_title('Daily Average Load with Potential Holidays/Anomalies (Red)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Rotate x-labels\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print anomaly dates\n",
    "print(\"Potential holidays/special days (lowest 5% load days):\")\n",
    "for date, load in anomalies.items():\n",
    "    print(f\"{date.strftime('%Y-%m-%d (%A)')}: {load:.0f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6abu4x4mjk",
   "metadata": {},
   "source": [
    "## 2. Time Series Decomposition\n",
    "\n",
    "Time series decomposition separates data into trend, seasonal, and residual components. This helps understand the underlying structure and is crucial for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4knbqjkw7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition\n",
    "# Using one month of data for clearer visualization\n",
    "july_2023 = load_data['2023-07']['load']\n",
    "\n",
    "# Additive decomposition (appropriate when seasonal fluctuations are relatively constant)\n",
    "decomposition = seasonal_decompose(july_2023, model='additive', period=24)\n",
    "\n",
    "# Create decomposition plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "# Original series\n",
    "july_2023.plot(ax=axes[0], title='Original Load Series - July 2023')\n",
    "axes[0].set_ylabel('Load (MW)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend\n",
    "decomposition.trend.plot(ax=axes[1], title='Trend Component')\n",
    "axes[1].set_ylabel('Trend (MW)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component (24-hour cycle)')\n",
    "axes[2].set_ylabel('Seasonal (MW)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual\n",
    "decomposition.resid.plot(ax=axes[3], title='Residual Component')\n",
    "axes[3].set_ylabel('Residual (MW)')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze components\n",
    "print(\"Decomposition Analysis:\")\n",
    "print(f\"Trend range: {decomposition.trend.dropna().min():.0f} - {decomposition.trend.dropna().max():.0f} MW\")\n",
    "print(f\"Seasonal amplitude: {decomposition.seasonal.max() - decomposition.seasonal.min():.0f} MW\")\n",
    "print(f\"Residual std dev: {decomposition.resid.dropna().std():.0f} MW\")\n",
    "\n",
    "# Show one day of seasonal pattern\n",
    "one_day_seasonal = decomposition.seasonal[:24]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(24), one_day_seasonal.values, 'b-', linewidth=2, marker='o')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Seasonal Component (MW)')\n",
    "plt.title('24-Hour Seasonal Pattern from Decomposition')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(0, 24, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ytihlucp",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 2: Decomposition Analysis\n",
    ":class: tip\n",
    "\n",
    "Perform time series decomposition on:\n",
    "1. A winter month (January) and compare seasonal amplitude to summer\n",
    "2. A full year of data to see long-term trends\n",
    "3. Create a multiplicative decomposition and compare to additive\n",
    "\n",
    "What insights can you draw about load behavior in different seasons?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9pixps7032m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Perform decomposition analysis on different time periods\n",
    "\n",
    "# Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g31aouglzy",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Winter decomposition (January 2023)\n",
    "jan_2023 = load_data['2023-01']['load']\n",
    "winter_decomp = seasonal_decompose(jan_2023, model='additive', period=24)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "winter_decomp.seasonal[:48].plot(ax=ax, linewidth=2)\n",
    "ax.set_title('Winter (January) Seasonal Pattern - First 2 Days')\n",
    "ax.set_ylabel('Seasonal Component (MW)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Summer decomposition (July 2023) for comparison\n",
    "ax = axes[0, 1]\n",
    "decomposition.seasonal[:48].plot(ax=ax, linewidth=2, color='red')\n",
    "ax.set_title('Summer (July) Seasonal Pattern - First 2 Days')\n",
    "ax.set_ylabel('Seasonal Component (MW)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Full year decomposition to see long-term trends\n",
    "year_2023 = load_data['2023']['load']\n",
    "# Resample to daily for yearly decomposition\n",
    "daily_load = year_2023.resample('D').mean()\n",
    "year_decomp = seasonal_decompose(daily_load, model='additive', period=7)  # Weekly seasonality\n",
    "\n",
    "ax = axes[1, 0]\n",
    "year_decomp.trend.plot(ax=ax, linewidth=2)\n",
    "ax.set_title('2023 Trend Component (Daily Average)')\n",
    "ax.set_ylabel('Trend (MW)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Multiplicative vs Additive comparison\n",
    "july_mult = seasonal_decompose(july_2023, model='multiplicative', period=24)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "# Plot ratio of seasonal components\n",
    "additive_range = decomposition.seasonal.max() - decomposition.seasonal.min()\n",
    "multiplicative_range = (july_mult.seasonal.max() - july_mult.seasonal.min()) * july_2023.mean()\n",
    "\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Model': ['Additive', 'Multiplicative'],\n",
    "    'Seasonal Range (MW)': [additive_range, multiplicative_range]\n",
    "})\n",
    "ax.bar(comparison_data['Model'], comparison_data['Seasonal Range (MW)'], \n",
    "       color=['blue', 'orange'], edgecolor='black')\n",
    "ax.set_ylabel('Seasonal Range (MW)')\n",
    "ax.set_title('Additive vs Multiplicative Model Comparison')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print(\"Decomposition Insights:\")\n",
    "print(f\"Winter seasonal amplitude: {winter_decomp.seasonal.max() - winter_decomp.seasonal.min():.0f} MW\")\n",
    "print(f\"Summer seasonal amplitude: {decomposition.seasonal.max() - decomposition.seasonal.min():.0f} MW\")\n",
    "print(f\"Ratio (Summer/Winter): {(decomposition.seasonal.max() - decomposition.seasonal.min()) / (winter_decomp.seasonal.max() - winter_decomp.seasonal.min()):.2f}\")\n",
    "print(f\"\\nYearly trend shows {year_decomp.trend.iloc[-1] - year_decomp.trend.iloc[0]:.0f} MW change over 2023\")\n",
    "print(f\"Multiplicative model suggests {(july_mult.seasonal.max() / july_mult.seasonal.min() - 1) * 100:.1f}% variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8ex4jhz3p",
   "metadata": {},
   "source": [
    "## 3. Autocorrelation and Lag Analysis\n",
    "\n",
    "Autocorrelation measures how current values relate to past values. This is crucial for understanding temporal dependencies and selecting appropriate forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfwbyoqbalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze autocorrelation for load data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Use one week of data for clarity\n",
    "week_load = load_data['2023-07-01':'2023-07-07']['load']\n",
    "\n",
    "# 1. Autocorrelation Function (ACF)\n",
    "ax = axes[0, 0]\n",
    "plot_acf(week_load, lags=72, ax=ax)  # 3 days of lags\n",
    "ax.set_title('Autocorrelation Function - Load Data')\n",
    "ax.set_xlabel('Lag (hours)')\n",
    "\n",
    "# 2. Partial Autocorrelation Function (PACF)\n",
    "ax = axes[0, 1]\n",
    "plot_pacf(week_load, lags=72, ax=ax)\n",
    "ax.set_title('Partial Autocorrelation Function - Load Data')\n",
    "ax.set_xlabel('Lag (hours)')\n",
    "\n",
    "# 3. Cross-correlation: Load vs Temperature\n",
    "ax = axes[1, 0]\n",
    "# Calculate cross-correlation manually\n",
    "max_lag = 48\n",
    "correlations = []\n",
    "lags = range(-max_lag, max_lag + 1)\n",
    "\n",
    "for lag in lags:\n",
    "    if lag < 0:\n",
    "        corr = week_load.iloc[:lag].corr(week_load.iloc[-lag:].index.map(lambda x: week_data.loc[x, 'temperature']))\n",
    "    elif lag > 0:\n",
    "        corr = week_load.iloc[lag:].corr(week_load.iloc[:-lag].index.map(lambda x: week_data.loc[x, 'temperature']))\n",
    "    else:\n",
    "        corr = week_load.corr(week_data['temperature'])\n",
    "    correlations.append(corr)\n",
    "\n",
    "# Simplified cross-correlation\n",
    "temp_week = week_data['temperature']\n",
    "cross_corr = [week_load.shift(lag).corr(temp_week) for lag in range(-24, 25)]\n",
    "lag_hours = range(-24, 25)\n",
    "\n",
    "ax.stem(lag_hours, cross_corr, basefmt=' ')\n",
    "ax.set_xlabel('Lag (hours)')\n",
    "ax.set_ylabel('Correlation')\n",
    "ax.set_title('Cross-correlation: Load vs Temperature')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "\n",
    "# 4. Lag plot for visual inspection\n",
    "ax = axes[1, 1]\n",
    "pd.plotting.lag_plot(week_load.values, lag=24, ax=ax)\n",
    "ax.set_title('24-hour Lag Plot')\n",
    "ax.set_xlabel('Load(t)')\n",
    "ax.set_ylabel('Load(t+24)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display key correlations\n",
    "print(\"Key Autocorrelations:\")\n",
    "print(f\"24-hour lag (daily): {week_load.autocorr(lag=24):.3f}\")\n",
    "print(f\"168-hour lag (weekly): {load_data['2023-07']['load'].autocorr(lag=168):.3f}\")\n",
    "print(f\"1-hour lag: {week_load.autocorr(lag=1):.3f}\")\n",
    "\n",
    "# Stationarity test\n",
    "result = adfuller(week_load)\n",
    "print(f\"\\nAugmented Dickey-Fuller Test:\")\n",
    "print(f\"Test Statistic: {result[0]:.4f}\")\n",
    "print(f\"p-value: {result[1]:.4f}\")\n",
    "print(f\"Series is {'stationary' if result[1] < 0.05 else 'non-stationary'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53bvwe4f7j",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3: Lag Analysis for Forecasting\n",
    ":class: tip\n",
    "\n",
    "1. Calculate autocorrelations for different times of day (morning peak vs overnight)\n",
    "2. Find the optimal lag for temperature as a predictor of load\n",
    "3. Compare weekday vs weekend autocorrelation patterns\n",
    "4. Identify which lags would be most useful for a forecasting model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ow6gr6nzv3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Analyze lag patterns for forecasting\n",
    "\n",
    "# Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156jbkbyn68l",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# 1. Autocorrelation by time of day\n",
    "morning_peak = load_data[(load_data.index.hour >= 7) & (load_data.index.hour <= 9)]['load']\n",
    "overnight = load_data[(load_data.index.hour >= 1) & (load_data.index.hour <= 4)]['load']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Morning peak ACF\n",
    "ax = axes[0, 0]\n",
    "plot_acf(morning_peak[:500], lags=48, ax=ax)\n",
    "ax.set_title('ACF - Morning Peak Hours (7-9 AM)')\n",
    "ax.set_xlabel('Lag (hours)')\n",
    "\n",
    "# Overnight ACF\n",
    "ax = axes[0, 1]\n",
    "plot_acf(overnight[:500], lags=48, ax=ax)\n",
    "ax.set_title('ACF - Overnight Hours (1-4 AM)')\n",
    "ax.set_xlabel('Lag (hours)')\n",
    "\n",
    "# 2. Temperature lag analysis\n",
    "ax = axes[1, 0]\n",
    "lag_range = range(0, 13)  # 0-12 hours\n",
    "temp_corr = []\n",
    "\n",
    "for lag in lag_range:\n",
    "    # Shift temperature backwards (temperature leads load)\n",
    "    corr = load_data['load'].corr(load_data['temperature'].shift(-lag))\n",
    "    temp_corr.append(corr)\n",
    "\n",
    "ax.bar(lag_range, temp_corr, color='orange', edgecolor='black')\n",
    "ax.set_xlabel('Temperature Lag (hours ahead)')\n",
    "ax.set_ylabel('Correlation with Load')\n",
    "ax.set_title('Temperature-Load Correlation by Lag')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "optimal_lag = lag_range[np.argmax(np.abs(temp_corr))]\n",
    "ax.axvline(x=optimal_lag, color='red', linestyle='--', label=f'Optimal lag: {optimal_lag}h')\n",
    "ax.legend()\n",
    "\n",
    "# 3. Weekday vs Weekend patterns\n",
    "ax = axes[1, 1]\n",
    "weekday_data = load_data[load_data['weekday'] < 5]['load']\n",
    "weekend_data = load_data[load_data['weekday'] >= 5]['load']\n",
    "\n",
    "lags_to_check = [1, 24, 48, 168]  # 1h, 1d, 2d, 1w\n",
    "weekday_acf = [weekday_data.autocorr(lag=lag) for lag in lags_to_check]\n",
    "weekend_acf = [weekend_data.autocorr(lag=lag) for lag in lags_to_check]\n",
    "\n",
    "x = np.arange(len(lags_to_check))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, weekday_acf, width, label='Weekday', color='blue', edgecolor='black')\n",
    "ax.bar(x + width/2, weekend_acf, width, label='Weekend', color='red', edgecolor='black')\n",
    "ax.set_xlabel('Lag')\n",
    "ax.set_ylabel('Autocorrelation')\n",
    "ax.set_title('Weekday vs Weekend Autocorrelations')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['1h', '24h', '48h', '168h'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Identify best lags for forecasting\n",
    "print(\"Lag Analysis Summary:\")\n",
    "print(f\"Morning peak shows stronger persistence than overnight\")\n",
    "print(f\"Optimal temperature lag: {optimal_lag} hours (temperature leads load)\")\n",
    "print(f\"Weekend patterns show weaker 24h correlation ({weekend_acf[1]:.3f}) vs weekday ({weekday_acf[1]:.3f})\")\n",
    "\n",
    "print(\"\\nRecommended lags for forecasting model:\")\n",
    "print(\"- Lag 1 (1 hour): Captures short-term persistence\")\n",
    "print(\"- Lag 24 (1 day): Captures daily patterns\")\n",
    "print(\"- Lag 168 (1 week): Captures weekly patterns\")\n",
    "print(f\"- Temperature with {optimal_lag}-hour lead\")\n",
    "print(\"- Consider separate models for weekday/weekend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fjzkos5i9st",
   "metadata": {},
   "source": [
    "## 4. Basic Forecasting Methods\n",
    "\n",
    "Now let's implement practical forecasting methods commonly used in power system operations. We'll start with simple methods and progress to more sophisticated approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5zz22v2rsfv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement basic forecasting methods\n",
    "# Use October 2023 data for training, forecast first week of November\n",
    "\n",
    "# Prepare data\n",
    "train_data = load_data['2023-10']\n",
    "test_data = load_data['2023-11-01':'2023-11-07']\n",
    "\n",
    "# Store forecasts\n",
    "forecasts = pd.DataFrame(index=test_data.index)\n",
    "forecasts['actual'] = test_data['load']\n",
    "\n",
    "# Method 1: Naive (persistence) forecast\n",
    "forecasts['naive'] = train_data['load'].iloc[-1]\n",
    "\n",
    "# Method 2: Yesterday same time\n",
    "forecasts['yesterday'] = np.nan\n",
    "for i in range(len(test_data)):\n",
    "    if i >= 24:\n",
    "        forecasts['yesterday'].iloc[i] = forecasts['actual'].iloc[i-24]\n",
    "    else:\n",
    "        # Use last days of training\n",
    "        forecasts['yesterday'].iloc[i] = train_data['load'].iloc[-(24-i)]\n",
    "\n",
    "# Method 3: Last week same time\n",
    "forecasts['last_week'] = np.nan\n",
    "for i in range(len(test_data)):\n",
    "    forecasts['last_week'].iloc[i] = train_data['load'].iloc[-(168-i)]\n",
    "\n",
    "# Method 4: Moving average (24-hour)\n",
    "window_size = 24\n",
    "forecasts['ma_24h'] = np.nan\n",
    "# Initialize with training data\n",
    "recent_values = list(train_data['load'].iloc[-window_size:])\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    forecasts['ma_24h'].iloc[i] = np.mean(recent_values)\n",
    "    # Update window with actual value (in practice, would use forecast)\n",
    "    recent_values.pop(0)\n",
    "    recent_values.append(test_data['load'].iloc[i])\n",
    "\n",
    "# Method 5: Simple regression with temperature\n",
    "# Create features\n",
    "train_features = pd.DataFrame({\n",
    "    'hour': train_data.index.hour,\n",
    "    'weekday': train_data.index.weekday,\n",
    "    'temperature': train_data['temperature'],\n",
    "    'temp_squared': train_data['temperature'] ** 2\n",
    "})\n",
    "train_target = train_data['load']\n",
    "\n",
    "test_features = pd.DataFrame({\n",
    "    'hour': test_data.index.hour,\n",
    "    'weekday': test_data.index.weekday,\n",
    "    'temperature': test_data['temperature'],\n",
    "    'temp_squared': test_data['temperature'] ** 2\n",
    "})\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(train_features, train_target)\n",
    "forecasts['regression'] = model.predict(test_features)\n",
    "\n",
    "# Method 6: Similar day approach\n",
    "forecasts['similar_day'] = np.nan\n",
    "for i, timestamp in enumerate(test_data.index):\n",
    "    # Find similar days (same hour, same day of week)\n",
    "    similar_mask = (train_data.index.hour == timestamp.hour) & \\\n",
    "                   (train_data.index.weekday == timestamp.weekday())\n",
    "    similar_loads = train_data.loc[similar_mask, 'load']\n",
    "    \n",
    "    if len(similar_loads) > 0:\n",
    "        forecasts['similar_day'].iloc[i] = similar_loads.mean()\n",
    "\n",
    "# Visualize first 48 hours of forecasts\n",
    "plt.figure(figsize=(14, 8))\n",
    "hours_to_plot = 48\n",
    "time_index = forecasts.index[:hours_to_plot]\n",
    "\n",
    "plt.plot(time_index, forecasts['actual'][:hours_to_plot], 'k-', linewidth=2, label='Actual')\n",
    "plt.plot(time_index, forecasts['yesterday'][:hours_to_plot], 'b--', label='Yesterday')\n",
    "plt.plot(time_index, forecasts['last_week'][:hours_to_plot], 'g--', label='Last Week')\n",
    "plt.plot(time_index, forecasts['ma_24h'][:hours_to_plot], 'r--', label='24h MA')\n",
    "plt.plot(time_index, forecasts['regression'][:hours_to_plot], 'm--', label='Regression')\n",
    "plt.plot(time_index, forecasts['similar_day'][:hours_to_plot], 'c--', label='Similar Day')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.title('Comparison of Basic Forecasting Methods (First 48 Hours)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors for each method\n",
    "print(\"Forecast Method Performance (Full Week):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "methods = ['naive', 'yesterday', 'last_week', 'ma_24h', 'regression', 'similar_day']\n",
    "for method in methods:\n",
    "    mae = mean_absolute_error(forecasts['actual'], forecasts[method])\n",
    "    rmse = np.sqrt(mean_squared_error(forecasts['actual'], forecasts[method]))\n",
    "    mape = np.mean(np.abs((forecasts['actual'] - forecasts[method]) / forecasts['actual'])) * 100\n",
    "    \n",
    "    print(f\"{method.ljust(15)} MAE: {mae:6.1f} MW  RMSE: {rmse:6.1f} MW  MAPE: {mape:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "szryqje3nzc",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 4: Implement a Day-Ahead Peak Load Forecast\n",
    ":class: tip\n",
    "\n",
    "Create a forecasting system that predicts tomorrow's peak load (maximum hourly load). Your system should:\n",
    "\n",
    "1. Use multiple forecasting methods\n",
    "2. Consider temperature forecasts\n",
    "3. Account for day-of-week effects\n",
    "4. Combine forecasts using weighted averaging\n",
    "5. Report the predicted peak hour and magnitude\n",
    "\n",
    "Test your system on multiple days and evaluate accuracy.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7zmvtpem3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "# Implement day-ahead peak load forecasting\n",
    "\n",
    "# Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dlrs6kl4dtd",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def forecast_peak_load(train_data, forecast_date, temperature_forecast=None):\n",
    "    \"\"\"\n",
    "    Forecast peak load for a given date using multiple methods\n",
    "    \n",
    "    Parameters:\n",
    "    train_data: Historical load data\n",
    "    forecast_date: Date to forecast (datetime)\n",
    "    temperature_forecast: Optional temperature forecast for the day\n",
    "    \n",
    "    Returns:\n",
    "    dict with peak_hour, peak_load, and method details\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get day of week\n",
    "    dow = forecast_date.weekday()\n",
    "    \n",
    "    # Method 1: Historical average for same day of week\n",
    "    hist_peaks = []\n",
    "    for d in pd.date_range(end=forecast_date - pd.Timedelta(days=1), periods=28, freq='D'):\n",
    "        if d.weekday() == dow and d in train_data.index:\n",
    "            daily_data = train_data[d.strftime('%Y-%m-%d')]\n",
    "            if len(daily_data) > 0:\n",
    "                peak_hour = daily_data['load'].idxmax().hour\n",
    "                peak_value = daily_data['load'].max()\n",
    "                hist_peaks.append((peak_hour, peak_value))\n",
    "    \n",
    "    if hist_peaks:\n",
    "        avg_peak_hour = int(np.mean([p[0] for p in hist_peaks]))\n",
    "        method1_peak = np.mean([p[1] for p in hist_peaks])\n",
    "    else:\n",
    "        avg_peak_hour = 17  # Default peak hour\n",
    "        method1_peak = train_data['load'].mean() * 1.2\n",
    "    \n",
    "    # Method 2: Last week same day\n",
    "    last_week_date = forecast_date - pd.Timedelta(days=7)\n",
    "    if last_week_date.strftime('%Y-%m-%d') in train_data.index:\n",
    "        last_week_data = train_data[last_week_date.strftime('%Y-%m-%d')]\n",
    "        method2_peak = last_week_data['load'].max()\n",
    "        method2_hour = last_week_data['load'].idxmax().hour\n",
    "    else:\n",
    "        method2_peak = method1_peak\n",
    "        method2_hour = avg_peak_hour\n",
    "    \n",
    "    # Method 3: Regression-based\n",
    "    # Prepare features for peak hours (typically 16-20)\n",
    "    peak_hours = range(16, 21)\n",
    "    features = []\n",
    "    \n",
    "    for hour in peak_hours:\n",
    "        feature = {\n",
    "            'hour': hour,\n",
    "            'weekday': dow,\n",
    "            'is_weekend': 1 if dow >= 5 else 0,\n",
    "            'month': forecast_date.month,\n",
    "        }\n",
    "        \n",
    "        # Add temperature if available\n",
    "        if temperature_forecast is not None:\n",
    "            feature['temp'] = temperature_forecast.get(hour, 20)  # Default 20Â°C\n",
    "            feature['temp_sq'] = feature['temp'] ** 2\n",
    "        else:\n",
    "            # Use historical average temperature\n",
    "            hist_temps = train_data[(train_data.index.hour == hour) & \n",
    "                                   (train_data.index.weekday == dow)]['temperature']\n",
    "            feature['temp'] = hist_temps.mean() if len(hist_temps) > 0 else 20\n",
    "            feature['temp_sq'] = feature['temp'] ** 2\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    # Train model on historical peak hour data\n",
    "    train_features = []\n",
    "    train_targets = []\n",
    "    \n",
    "    for idx in train_data.index:\n",
    "        if idx.hour in peak_hours:\n",
    "            train_features.append({\n",
    "                'hour': idx.hour,\n",
    "                'weekday': idx.weekday(),\n",
    "                'is_weekend': 1 if idx.weekday() >= 5 else 0,\n",
    "                'month': idx.month,\n",
    "                'temp': train_data.loc[idx, 'temperature'],\n",
    "                'temp_sq': train_data.loc[idx, 'temperature'] ** 2\n",
    "            })\n",
    "            train_targets.append(train_data.loc[idx, 'load'])\n",
    "    \n",
    "    X_train = pd.DataFrame(train_features)\n",
    "    y_train = np.array(train_targets)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict for peak hours\n",
    "    X_forecast = pd.DataFrame(features)\n",
    "    predictions = model.predict(X_forecast)\n",
    "    \n",
    "    method3_idx = np.argmax(predictions)\n",
    "    method3_peak = predictions[method3_idx]\n",
    "    method3_hour = peak_hours[method3_idx]\n",
    "    \n",
    "    # Method 4: Trend-adjusted similar day\n",
    "    # Find trend over last 7 days\n",
    "    recent_peaks = []\n",
    "    for i in range(1, 8):\n",
    "        date = forecast_date - pd.Timedelta(days=i)\n",
    "        if date.strftime('%Y-%m-%d') in train_data.index:\n",
    "            daily_max = train_data[date.strftime('%Y-%m-%d')]['load'].max()\n",
    "            recent_peaks.append(daily_max)\n",
    "    \n",
    "    if len(recent_peaks) >= 3:\n",
    "        trend = np.polyfit(range(len(recent_peaks)), recent_peaks, 1)[0]\n",
    "        method4_peak = method2_peak + trend * 7  # Extrapolate trend\n",
    "    else:\n",
    "        method4_peak = method2_peak\n",
    "    \n",
    "    # Combine forecasts with weights\n",
    "    weights = {\n",
    "        'historical': 0.2,\n",
    "        'last_week': 0.3,\n",
    "        'regression': 0.4,\n",
    "        'trend': 0.1\n",
    "    }\n",
    "    \n",
    "    combined_peak = (\n",
    "        weights['historical'] * method1_peak +\n",
    "        weights['last_week'] * method2_peak +\n",
    "        weights['regression'] * method3_peak +\n",
    "        weights['trend'] * method4_peak\n",
    "    )\n",
    "    \n",
    "    # Determine most likely peak hour (weighted average)\n",
    "    hour_votes = {\n",
    "        avg_peak_hour: weights['historical'],\n",
    "        method2_hour: weights['last_week'],\n",
    "        method3_hour: weights['regression']\n",
    "    }\n",
    "    \n",
    "    peak_hour = max(hour_votes, key=hour_votes.get)\n",
    "    \n",
    "    return {\n",
    "        'date': forecast_date,\n",
    "        'peak_hour': peak_hour,\n",
    "        'peak_load': combined_peak,\n",
    "        'methods': {\n",
    "            'historical': method1_peak,\n",
    "            'last_week': method2_peak,\n",
    "            'regression': method3_peak,\n",
    "            'trend_adjusted': method4_peak\n",
    "        },\n",
    "        'weights': weights\n",
    "    }\n",
    "\n",
    "# Test the system on multiple days\n",
    "test_dates = pd.date_range(start='2023-11-01', end='2023-11-07', freq='D')\n",
    "results = []\n",
    "\n",
    "for test_date in test_dates:\n",
    "    # Use data up to day before\n",
    "    train_end = test_date - pd.Timedelta(days=1)\n",
    "    train_subset = load_data[:train_end]\n",
    "    \n",
    "    # Forecast\n",
    "    forecast = forecast_peak_load(train_subset, test_date)\n",
    "    \n",
    "    # Get actual\n",
    "    actual_data = load_data[test_date.strftime('%Y-%m-%d')]\n",
    "    actual_peak = actual_data['load'].max()\n",
    "    actual_peak_hour = actual_data['load'].idxmax().hour\n",
    "    \n",
    "    # Store results\n",
    "    forecast['actual_peak'] = actual_peak\n",
    "    forecast['actual_peak_hour'] = actual_peak_hour\n",
    "    forecast['error_mw'] = forecast['peak_load'] - actual_peak\n",
    "    forecast['error_pct'] = (forecast['peak_load'] - actual_peak) / actual_peak * 100\n",
    "    forecast['hour_error'] = abs(forecast['peak_hour'] - actual_peak_hour)\n",
    "    \n",
    "    results.append(forecast)\n",
    "\n",
    "# Display results\n",
    "print(\"Peak Load Forecast Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Date':<12} {'Forecast':<10} {'Actual':<10} {'Error':<10} {'Hour Forecast':<15} {'Hour Actual':<12} {'Hour Error':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['date'].strftime('%Y-%m-%d'):<12} \"\n",
    "          f\"{r['peak_load']:>8.0f} MW  \"\n",
    "          f\"{r['actual_peak']:>8.0f} MW  \"\n",
    "          f\"{r['error_mw']:>7.0f} MW  \"\n",
    "          f\"{r['peak_hour']:>13d}:00  \"\n",
    "          f\"{r['actual_peak_hour']:>10d}:00  \"\n",
    "          f\"{r['hour_error']:>8d}h\")\n",
    "\n",
    "# Summary statistics\n",
    "errors_mw = [r['error_mw'] for r in results]\n",
    "errors_pct = [r['error_pct'] for r in results]\n",
    "hour_errors = [r['hour_error'] for r in results]\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"MAE: {np.mean(np.abs(errors_mw)):.1f} MW\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean(np.array(errors_mw)**2)):.1f} MW\")\n",
    "print(f\"MAPE: {np.mean(np.abs(errors_pct)):.2f}%\")\n",
    "print(f\"Average hour error: {np.mean(hour_errors):.1f} hours\")\n",
    "\n",
    "# Visualize one day's forecast breakdown\n",
    "example_forecast = results[0]\n",
    "methods = list(example_forecast['methods'].keys())\n",
    "values = list(example_forecast['methods'].values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(methods, values, color=['blue', 'green', 'orange', 'red'], edgecolor='black')\n",
    "plt.axhline(y=example_forecast['peak_load'], color='purple', linestyle='--', \n",
    "            linewidth=2, label=f\"Combined: {example_forecast['peak_load']:.0f} MW\")\n",
    "plt.axhline(y=example_forecast['actual_peak'], color='black', linestyle='-', \n",
    "            linewidth=2, label=f\"Actual: {example_forecast['actual_peak']:.0f} MW\")\n",
    "\n",
    "plt.xlabel('Forecasting Method')\n",
    "plt.ylabel('Peak Load (MW)')\n",
    "plt.title(f\"Peak Load Forecast Breakdown - {example_forecast['date'].strftime('%Y-%m-%d')}\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "             f'{value:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bisxhqac5g",
   "metadata": {},
   "source": [
    "## 5. Forecast Evaluation\n",
    "\n",
    "Proper evaluation of forecast accuracy is crucial for system operators to understand the reliability of their predictions and plan appropriate reserves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aq7xs839l1q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive forecast evaluation\n",
    "# Let's evaluate forecasts at different time horizons\n",
    "\n",
    "def evaluate_forecasts_by_horizon(forecasts, actuals, horizons=[1, 6, 12, 24]):\n",
    "    \"\"\"\n",
    "    Evaluate forecast accuracy by time horizon\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for h in horizons:\n",
    "        # Get forecast/actual pairs for this horizon\n",
    "        forecast_h = forecasts.iloc[::h]\n",
    "        actual_h = actuals.iloc[::h]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        errors = forecast_h - actual_h\n",
    "        mae = np.mean(np.abs(errors))\n",
    "        rmse = np.sqrt(np.mean(errors**2))\n",
    "        mape = np.mean(np.abs(errors / actual_h)) * 100\n",
    "        \n",
    "        # Bias (systematic over/under forecasting)\n",
    "        bias = np.mean(errors)\n",
    "        \n",
    "        results[h] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'Bias': bias,\n",
    "            'Count': len(errors)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate different methods by horizon\n",
    "horizon_results = {}\n",
    "for method in methods:\n",
    "    horizon_results[method] = evaluate_forecasts_by_horizon(\n",
    "        forecasts[method], \n",
    "        forecasts['actual']\n",
    "    )\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. MAE by horizon\n",
    "ax = axes[0, 0]\n",
    "horizons = [1, 6, 12, 24]\n",
    "for method in ['yesterday', 'last_week', 'regression']:\n",
    "    mae_values = [horizon_results[method][h]['MAE'] for h in horizons]\n",
    "    ax.plot(horizons, mae_values, marker='o', label=method)\n",
    "\n",
    "ax.set_xlabel('Forecast Horizon (hours)')\n",
    "ax.set_ylabel('MAE (MW)')\n",
    "ax.set_title('Mean Absolute Error by Forecast Horizon')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MAPE by time of day\n",
    "ax = axes[0, 1]\n",
    "hourly_mape = {}\n",
    "for hour in range(24):\n",
    "    hour_mask = forecasts.index.hour == hour\n",
    "    hourly_mape[hour] = {}\n",
    "    \n",
    "    for method in ['yesterday', 'last_week', 'regression']:\n",
    "        if hour_mask.sum() > 0:\n",
    "            errors = np.abs((forecasts.loc[hour_mask, method] - \n",
    "                           forecasts.loc[hour_mask, 'actual']) / \n",
    "                          forecasts.loc[hour_mask, 'actual'])\n",
    "            hourly_mape[hour][method] = np.mean(errors) * 100\n",
    "\n",
    "hours = list(range(24))\n",
    "for method in ['yesterday', 'last_week', 'regression']:\n",
    "    mape_by_hour = [hourly_mape[h][method] for h in hours]\n",
    "    ax.plot(hours, mape_by_hour, marker='o', markersize=4, label=method)\n",
    "\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('MAPE (%)')\n",
    "ax.set_title('Forecast Accuracy by Time of Day')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(0, 24, 3))\n",
    "\n",
    "# 3. Error distribution\n",
    "ax = axes[1, 0]\n",
    "errors_regression = forecasts['regression'] - forecasts['actual']\n",
    "ax.hist(errors_regression, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Forecast Error (MW)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Forecast Errors (Regression Method)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add statistics\n",
    "textstr = f'Mean: {np.mean(errors_regression):.1f} MW\\n'\n",
    "textstr += f'Std: {np.std(errors_regression):.1f} MW\\n'\n",
    "textstr += f'Skew: {errors_regression.skew():.2f}'\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, \n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 4. Forecast vs Actual scatter plot\n",
    "ax = axes[1, 1]\n",
    "sample_size = 500  # Plot subset for clarity\n",
    "indices = np.random.choice(len(forecasts), sample_size, replace=False)\n",
    "\n",
    "ax.scatter(forecasts.iloc[indices]['actual'], \n",
    "          forecasts.iloc[indices]['regression'], \n",
    "          alpha=0.5, s=20)\n",
    "\n",
    "# Add perfect forecast line\n",
    "min_val = forecasts['actual'].min()\n",
    "max_val = forecasts['actual'].max()\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Forecast')\n",
    "\n",
    "# Add regression line\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, _, _ = stats.linregress(\n",
    "    forecasts.iloc[indices]['actual'], \n",
    "    forecasts.iloc[indices]['regression']\n",
    ")\n",
    "x_reg = np.array([min_val, max_val])\n",
    "y_reg = slope * x_reg + intercept\n",
    "ax.plot(x_reg, y_reg, 'g-', linewidth=2, label=f'Fit (RÂ²={r_value**2:.3f})')\n",
    "\n",
    "ax.set_xlabel('Actual Load (MW)')\n",
    "ax.set_ylabel('Forecast Load (MW)')\n",
    "ax.set_title('Forecast vs Actual Load (Regression Method)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Peak hour accuracy analysis\n",
    "print(\"\\nPeak Hour Forecast Accuracy:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "peak_hours = [17, 18, 19]  # Typical peak hours\n",
    "peak_mask = forecasts.index.hour.isin(peak_hours)\n",
    "off_peak_mask = ~peak_mask\n",
    "\n",
    "for method in ['yesterday', 'last_week', 'regression']:\n",
    "    peak_mape = np.mean(np.abs((forecasts.loc[peak_mask, method] - \n",
    "                               forecasts.loc[peak_mask, 'actual']) / \n",
    "                              forecasts.loc[peak_mask, 'actual'])) * 100\n",
    "    \n",
    "    off_peak_mape = np.mean(np.abs((forecasts.loc[off_peak_mask, method] - \n",
    "                                   forecasts.loc[off_peak_mask, 'actual']) / \n",
    "                                  forecasts.loc[off_peak_mask, 'actual'])) * 100\n",
    "    \n",
    "    print(f\"{method.ljust(15)} Peak MAPE: {peak_mape:5.2f}%  Off-Peak MAPE: {off_peak_mape:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0kypjty3a9",
   "metadata": {},
   "source": [
    "## 6. Practical Considerations\n",
    "\n",
    "Real-world forecasting faces many challenges beyond statistical modeling. Let's explore common issues and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dpf5u30q2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling practical forecasting challenges\n",
    "\n",
    "# 1. Missing data handling\n",
    "# Introduce some missing values\n",
    "load_with_missing = load_data.copy()\n",
    "missing_mask = np.random.random(len(load_data)) < 0.02  # 2% missing\n",
    "load_with_missing.loc[missing_mask, 'load'] = np.nan\n",
    "\n",
    "print(\"Missing Data Analysis:\")\n",
    "print(f\"Total missing values: {load_with_missing['load'].isna().sum()}\")\n",
    "print(f\"Percentage missing: {load_with_missing['load'].isna().mean() * 100:.1f}%\")\n",
    "\n",
    "# Different imputation strategies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Show a week with missing data\n",
    "week_missing = load_with_missing['2023-03-01':'2023-03-07']['load']\n",
    "\n",
    "# Method 1: Forward fill\n",
    "ax = axes[0, 0]\n",
    "filled_forward = week_missing.fillna(method='ffill')\n",
    "week_missing.plot(ax=ax, style='b-', label='Original', alpha=0.7)\n",
    "filled_forward.plot(ax=ax, style='r--', label='Forward Fill', alpha=0.7)\n",
    "ax.scatter(week_missing[week_missing.isna()].index, \n",
    "          filled_forward[week_missing.isna()], \n",
    "          color='red', s=50, zorder=5)\n",
    "ax.set_title('Forward Fill Method')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Method 2: Interpolation\n",
    "ax = axes[0, 1]\n",
    "filled_interp = week_missing.interpolate(method='time')\n",
    "week_missing.plot(ax=ax, style='b-', label='Original', alpha=0.7)\n",
    "filled_interp.plot(ax=ax, style='g--', label='Interpolation', alpha=0.7)\n",
    "ax.scatter(week_missing[week_missing.isna()].index, \n",
    "          filled_interp[week_missing.isna()], \n",
    "          color='green', s=50, zorder=5)\n",
    "ax.set_title('Time-based Interpolation')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Method 3: Similar hour from previous week\n",
    "ax = axes[1, 0]\n",
    "filled_similar = week_missing.copy()\n",
    "for idx in week_missing[week_missing.isna()].index:\n",
    "    similar_idx = idx - pd.Timedelta(days=7)\n",
    "    if similar_idx in load_with_missing.index:\n",
    "        filled_similar[idx] = load_with_missing.loc[similar_idx, 'load']\n",
    "\n",
    "week_missing.plot(ax=ax, style='b-', label='Original', alpha=0.7)\n",
    "filled_similar.plot(ax=ax, style='m--', label='Previous Week', alpha=0.7)\n",
    "ax.scatter(week_missing[week_missing.isna()].index, \n",
    "          filled_similar[week_missing.isna()], \n",
    "          color='magenta', s=50, zorder=5)\n",
    "ax.set_title('Similar Hour Previous Week')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Extreme weather event impact\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Simulate a heat wave\n",
    "normal_temp = 25  # Normal summer temperature\n",
    "heat_wave_start = pd.Timestamp('2023-07-15')\n",
    "heat_wave_end = pd.Timestamp('2023-07-20')\n",
    "\n",
    "# Extract heat wave period\n",
    "heat_wave_data = load_data[heat_wave_start:heat_wave_end].copy()\n",
    "normal_week = load_data['2023-07-01':'2023-07-06'].copy()\n",
    "\n",
    "# Plot comparison\n",
    "hours = range(len(normal_week))\n",
    "ax.plot(hours, normal_week['load'], 'b-', label='Normal Week', linewidth=2)\n",
    "ax.plot(hours, heat_wave_data['load'][:len(hours)], 'r-', label='Heat Wave Week', linewidth=2)\n",
    "\n",
    "# Add temperature on secondary axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(hours, normal_week['temperature'], 'b--', alpha=0.5, label='Normal Temp')\n",
    "ax2.plot(hours, heat_wave_data['temperature'][:len(hours)], 'r--', alpha=0.5, label='Heat Wave Temp')\n",
    "ax2.set_ylabel('Temperature (Â°C)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax.set_xlabel('Hours')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.set_title('Load Impact During Extreme Weather Event')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Behind-the-meter solar impact\n",
    "print(\"\\nBehind-the-Meter Solar Impact Analysis:\")\n",
    "\n",
    "# Simulate solar reduction effect (simplified)\n",
    "solar_hours = (load_data.index.hour >= 10) & (load_data.index.hour <= 16)\n",
    "summer_days = load_data.index.month.isin([6, 7, 8])\n",
    "\n",
    "# Calculate apparent load reduction\n",
    "solar_mask = solar_hours & summer_days\n",
    "avg_reduction = load_data.loc[solar_mask, 'load'].mean() * 0.05  # 5% reduction\n",
    "\n",
    "print(f\"Average midday load without solar: {load_data.loc[solar_mask, 'load'].mean():.0f} MW\")\n",
    "print(f\"Estimated solar impact: {avg_reduction:.0f} MW\")\n",
    "print(f\"This represents hidden load that affects forecast accuracy\")\n",
    "\n",
    "# 4. Holiday detection and handling\n",
    "# Simple holiday detection based on low load\n",
    "daily_avg = load_data.resample('D')['load'].mean()\n",
    "threshold = daily_avg.quantile(0.05)\n",
    "potential_holidays = daily_avg[daily_avg < threshold]\n",
    "\n",
    "print(f\"\\nDetected {len(potential_holidays)} potential holidays/special days\")\n",
    "print(\"Examples:\")\n",
    "for date, load in list(potential_holidays.items())[:5]:\n",
    "    day_name = date.strftime('%A')\n",
    "    print(f\"  {date.strftime('%Y-%m-%d')} ({day_name}): {load:.0f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etgz4j4p6h",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 5: Build a Complete Forecasting System\n",
    ":class: tip\n",
    "\n",
    "Create a simple load forecasting system that:\n",
    "\n",
    "1. Handles missing data appropriately\n",
    "2. Detects and flags holidays/special events\n",
    "3. Combines multiple forecasting methods\n",
    "4. Provides uncertainty estimates\n",
    "5. Generates a forecast report\n",
    "\n",
    "Test your system with real-world scenarios including missing data and holidays.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "os6nnle1ia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your code here\n",
    "# Build a complete forecasting system\n",
    "\n",
    "# Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qli9tfdn92f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class LoadForecastingSystem:\n",
    "    \"\"\"\n",
    "    Complete load forecasting system with practical features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, historical_data):\n",
    "        self.data = historical_data.copy()\n",
    "        self.holidays = []\n",
    "        self.models = {}\n",
    "        \n",
    "    def detect_holidays(self, threshold_percentile=5):\n",
    "        \"\"\"Detect potential holidays based on low load\"\"\"\n",
    "        daily_avg = self.data.resample('D')['load'].mean()\n",
    "        threshold = daily_avg.quantile(threshold_percentile / 100)\n",
    "        self.holidays = daily_avg[daily_avg < threshold].index.tolist()\n",
    "        return self.holidays\n",
    "    \n",
    "    def handle_missing_data(self, method='interpolate'):\n",
    "        \"\"\"Handle missing values in the data\"\"\"\n",
    "        missing_count = self.data['load'].isna().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            if method == 'interpolate':\n",
    "                self.data['load'] = self.data['load'].interpolate(method='time')\n",
    "            elif method == 'forward_fill':\n",
    "                self.data['load'] = self.data['load'].fillna(method='ffill')\n",
    "            elif method == 'similar_hour':\n",
    "                # Fill with same hour from previous week\n",
    "                for idx in self.data[self.data['load'].isna()].index:\n",
    "                    prev_week = idx - pd.Timedelta(days=7)\n",
    "                    if prev_week in self.data.index:\n",
    "                        self.data.loc[idx, 'load'] = self.data.loc[prev_week, 'load']\n",
    "            \n",
    "            print(f\"Filled {missing_count} missing values using {method}\")\n",
    "    \n",
    "    def train_models(self, train_end_date):\n",
    "        \"\"\"Train multiple forecasting models\"\"\"\n",
    "        train_data = self.data[:train_end_date]\n",
    "        \n",
    "        # Model 1: Linear Regression\n",
    "        features = pd.DataFrame({\n",
    "            'hour': train_data.index.hour,\n",
    "            'weekday': train_data.index.weekday,\n",
    "            'month': train_data.index.month,\n",
    "            'is_weekend': (train_data.index.weekday >= 5).astype(int),\n",
    "            'temperature': train_data['temperature'],\n",
    "            'temp_squared': train_data['temperature'] ** 2\n",
    "        })\n",
    "        \n",
    "        self.models['regression'] = LinearRegression()\n",
    "        self.models['regression'].fit(features, train_data['load'])\n",
    "        \n",
    "        # Store training data for other methods\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def forecast(self, forecast_dates, confidence_level=0.95):\n",
    "        \"\"\"Generate forecasts with uncertainty estimates\"\"\"\n",
    "        results = pd.DataFrame(index=forecast_dates)\n",
    "        \n",
    "        # Check if forecast date is a holiday\n",
    "        results['is_holiday'] = results.index.normalize().isin(self.holidays)\n",
    "        \n",
    "        # Method 1: Regression\n",
    "        features = pd.DataFrame({\n",
    "            'hour': forecast_dates.hour,\n",
    "            'weekday': forecast_dates.weekday,\n",
    "            'month': forecast_dates.month,\n",
    "            'is_weekend': (forecast_dates.weekday >= 5).astype(int),\n",
    "            'temperature': 20,  # Default temperature\n",
    "            'temp_squared': 400\n",
    "        })\n",
    "        results['regression'] = self.models['regression'].predict(features)\n",
    "        \n",
    "        # Method 2: Similar day\n",
    "        similar_day_forecasts = []\n",
    "        for date in forecast_dates:\n",
    "            if results.loc[date, 'is_holiday']:\n",
    "                # Use average of past holidays\n",
    "                holiday_loads = []\n",
    "                for holiday in self.holidays:\n",
    "                    if holiday < date and date.hour in self.train_data.index.hour:\n",
    "                        holiday_data = self.train_data[holiday.strftime('%Y-%m-%d')]\n",
    "                        holiday_hour_data = holiday_data[holiday_data.index.hour == date.hour]\n",
    "                        if len(holiday_hour_data) > 0:\n",
    "                            holiday_loads.append(holiday_hour_data['load'].mean())\n",
    "                \n",
    "                similar_day_forecasts.append(np.mean(holiday_loads) if holiday_loads else results.loc[date, 'regression'] * 0.85)\n",
    "            else:\n",
    "                # Normal similar day logic\n",
    "                similar_days = self.train_data[\n",
    "                    (self.train_data.index.hour == date.hour) & \n",
    "                    (self.train_data.index.weekday == date.weekday())\n",
    "                ]['load']\n",
    "                similar_day_forecasts.append(similar_days.mean() if len(similar_days) > 0 else results.loc[date, 'regression'])\n",
    "        \n",
    "        results['similar_day'] = similar_day_forecasts\n",
    "        \n",
    "        # Method 3: Last week\n",
    "        last_week_forecasts = []\n",
    "        for date in forecast_dates:\n",
    "            last_week = date - pd.Timedelta(days=7)\n",
    "            if last_week in self.train_data.index:\n",
    "                last_week_forecasts.append(self.train_data.loc[last_week, 'load'])\n",
    "            else:\n",
    "                last_week_forecasts.append(results.loc[date, 'similar_day'])\n",
    "        \n",
    "        results['last_week'] = last_week_forecasts\n",
    "        \n",
    "        # Combine forecasts\n",
    "        if results['is_holiday'].any():\n",
    "            # Adjust weights for holidays\n",
    "            results.loc[results['is_holiday'], 'combined'] = (\n",
    "                0.2 * results.loc[results['is_holiday'], 'regression'] +\n",
    "                0.6 * results.loc[results['is_holiday'], 'similar_day'] +\n",
    "                0.2 * results.loc[results['is_holiday'], 'last_week']\n",
    "            )\n",
    "            results.loc[~results['is_holiday'], 'combined'] = (\n",
    "                0.4 * results.loc[~results['is_holiday'], 'regression'] +\n",
    "                0.3 * results.loc[~results['is_holiday'], 'similar_day'] +\n",
    "                0.3 * results.loc[~results['is_holiday'], 'last_week']\n",
    "            )\n",
    "        else:\n",
    "            results['combined'] = 0.4 * results['regression'] + 0.3 * results['similar_day'] + 0.3 * results['last_week']\n",
    "        \n",
    "        # Estimate uncertainty based on historical errors\n",
    "        # Calculate standard deviation of errors for similar conditions\n",
    "        uncertainty = []\n",
    "        for date in forecast_dates:\n",
    "            similar_conditions = self.train_data[\n",
    "                (self.train_data.index.hour == date.hour) & \n",
    "                (self.train_data.index.weekday == date.weekday())\n",
    "            ]['load']\n",
    "            \n",
    "            if len(similar_conditions) > 1:\n",
    "                std_dev = similar_conditions.std()\n",
    "            else:\n",
    "                std_dev = self.train_data['load'].std() * 0.1  # Default to 10% of overall std\n",
    "            \n",
    "            uncertainty.append(std_dev)\n",
    "        \n",
    "        results['uncertainty'] = uncertainty\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        results['lower_bound'] = results['combined'] - z_score * results['uncertainty']\n",
    "        results['upper_bound'] = results['combined'] + z_score * results['uncertainty']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, forecast_results, actual_data=None):\n",
    "        \"\"\"Generate a forecast report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"LOAD FORECAST REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(f\"Forecast Period: {forecast_results.index[0]} to {forecast_results.index[-1]}\")\n",
    "        report.append(f\"Number of Hours: {len(forecast_results)}\")\n",
    "        \n",
    "        # Holiday information\n",
    "        holiday_count = forecast_results['is_holiday'].sum()\n",
    "        if holiday_count > 0:\n",
    "            report.append(f\"\\nSpecial Days Detected: {holiday_count} hours\")\n",
    "            holiday_dates = forecast_results[forecast_results['is_holiday']].index.normalize().unique()\n",
    "            for date in holiday_dates:\n",
    "                report.append(f\"  - {date.strftime('%Y-%m-%d (%A)')}\")\n",
    "        \n",
    "        # Forecast summary\n",
    "        report.append(f\"\\nForecast Summary:\")\n",
    "        report.append(f\"  Average Load: {forecast_results['combined'].mean():.0f} MW\")\n",
    "        report.append(f\"  Peak Load: {forecast_results['combined'].max():.0f} MW\")\n",
    "        report.append(f\"  Peak Hour: {forecast_results['combined'].idxmax()}\")\n",
    "        report.append(f\"  Min Load: {forecast_results['combined'].min():.0f} MW\")\n",
    "        \n",
    "        # Uncertainty analysis\n",
    "        report.append(f\"\\nUncertainty Analysis (95% confidence):\")\n",
    "        report.append(f\"  Average Uncertainty: Â±{forecast_results['uncertainty'].mean():.0f} MW\")\n",
    "        report.append(f\"  Max Uncertainty: Â±{forecast_results['uncertainty'].max():.0f} MW\")\n",
    "        \n",
    "        # If actual data provided, calculate accuracy\n",
    "        if actual_data is not None:\n",
    "            errors = forecast_results['combined'] - actual_data['load']\n",
    "            mae = np.mean(np.abs(errors))\n",
    "            rmse = np.sqrt(np.mean(errors**2))\n",
    "            mape = np.mean(np.abs(errors / actual_data['load'])) * 100\n",
    "            \n",
    "            report.append(f\"\\nForecast Accuracy:\")\n",
    "            report.append(f\"  MAE: {mae:.1f} MW\")\n",
    "            report.append(f\"  RMSE: {rmse:.1f} MW\")\n",
    "            report.append(f\"  MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Test the system\n",
    "# Create data with some missing values\n",
    "test_data = load_data.copy()\n",
    "missing_indices = np.random.choice(test_data.index, size=100, replace=False)\n",
    "test_data.loc[missing_indices, 'load'] = np.nan\n",
    "\n",
    "# Initialize system\n",
    "forecast_system = LoadForecastingSystem(test_data)\n",
    "\n",
    "# Handle missing data\n",
    "forecast_system.handle_missing_data(method='interpolate')\n",
    "\n",
    "# Detect holidays\n",
    "holidays = forecast_system.detect_holidays()\n",
    "print(f\"Detected {len(holidays)} potential holidays\")\n",
    "\n",
    "# Train models\n",
    "train_end = pd.Timestamp('2023-10-31')\n",
    "forecast_system.train_models(train_end)\n",
    "\n",
    "# Generate forecast for first week of November\n",
    "forecast_dates = pd.date_range(start='2023-11-01', end='2023-11-07 23:00:00', freq='H')\n",
    "forecast_results = forecast_system.forecast(forecast_dates)\n",
    "\n",
    "# Get actual data for comparison\n",
    "actual_data = load_data['2023-11-01':'2023-11-07']\n",
    "\n",
    "# Generate report\n",
    "report = forecast_system.generate_report(forecast_results, actual_data)\n",
    "print(report)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Forecast with confidence intervals\n",
    "ax = axes[0]\n",
    "hours_to_plot = 72  # First 3 days\n",
    "time_index = forecast_results.index[:hours_to_plot]\n",
    "\n",
    "ax.plot(time_index, actual_data['load'][:hours_to_plot], 'k-', linewidth=2, label='Actual')\n",
    "ax.plot(time_index, forecast_results['combined'][:hours_to_plot], 'b-', linewidth=2, label='Forecast')\n",
    "ax.fill_between(time_index, \n",
    "                forecast_results['lower_bound'][:hours_to_plot],\n",
    "                forecast_results['upper_bound'][:hours_to_plot],\n",
    "                alpha=0.3, color='blue', label='95% Confidence')\n",
    "\n",
    "# Mark holidays\n",
    "holiday_mask = forecast_results['is_holiday'][:hours_to_plot]\n",
    "if holiday_mask.any():\n",
    "    ax.scatter(time_index[holiday_mask], \n",
    "              forecast_results.loc[holiday_mask, 'combined'][:hours_to_plot],\n",
    "              color='red', s=50, marker='*', zorder=5, label='Holiday Hours')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Load (MW)')\n",
    "ax.set_title('Load Forecast with Uncertainty Bands')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Forecast errors\n",
    "ax = axes[1]\n",
    "errors = forecast_results['combined'] - actual_data['load']\n",
    "ax.plot(forecast_results.index, errors, 'r-', alpha=0.7)\n",
    "ax.axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "ax.fill_between(forecast_results.index, errors, 0, \n",
    "                where=(errors > 0), color='red', alpha=0.3, label='Over-forecast')\n",
    "ax.fill_between(forecast_results.index, errors, 0, \n",
    "                where=(errors < 0), color='green', alpha=0.3, label='Under-forecast')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Forecast Error (MW)')\n",
    "ax.set_title('Forecast Errors Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6isnb2rne3e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we covered essential time series analysis techniques for power system forecasting:\n",
    "\n",
    "1. **Pattern Recognition**: Identified daily, weekly, and seasonal patterns in load data\n",
    "2. **Decomposition**: Separated time series into trend, seasonal, and residual components\n",
    "3. **Autocorrelation Analysis**: Determined optimal lags for forecasting models\n",
    "4. **Forecasting Methods**: Implemented multiple approaches from simple to sophisticated\n",
    "5. **Evaluation Metrics**: Applied industry-standard accuracy measures\n",
    "6. **Practical Challenges**: Addressed missing data, holidays, and extreme events\n",
    "\n",
    "These techniques form the foundation for operational forecasting in power systems. Real-world applications extend these concepts with machine learning models, weather integration, and ensemble methods.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Power system data exhibits strong temporal patterns driven by human behavior and weather\n",
    "- No single forecasting method works best for all situations\n",
    "- Combining multiple methods often improves accuracy\n",
    "- Special events require different treatment than normal days\n",
    "- Uncertainty quantification is as important as point forecasts\n",
    "- Data quality significantly impacts forecast accuracy\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further develop your forecasting skills:\n",
    "- Explore advanced methods like ARIMA and neural networks\n",
    "- Integrate real-time weather data\n",
    "- Study very short-term (minutes) and long-term (months) forecasting\n",
    "- Learn about probabilistic forecasting for renewable generation\n",
    "- Practice with real utility data when available"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
